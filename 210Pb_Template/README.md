

# Working with the Python Data Upload Template

This set of python scripts is intended to support the bulk upload of a set of records to Neotoma. It consists of three key steps:

1. Development of a data template (YAML and CSV)
2. Template validation
3. Data upload

Once these three steps are completed the uploader will push the template files to the `neotomaholding` database. This is a temporary database that is intended to hold data within the Neotoma Paleoecology Database system for access by Tilia. Tilia is then used to provide a final data check and upload of data to Neotoma proper.

## Template Development

The template uses a `yaml` format file, with the following general structure for each data element:

```yaml
apiVersion: neotoma v2.0
kind: Development
metadata:
  - column:  Site.name
    neotoma: ndb.sites.sitename  
    vocab: False
    repeat: True
    type: string
    ordered: False
```

The template is used to link the template CSV file (the file that will be generated by the upload team) to the Neotoma database. It is a form of cross-walk between the upload team and the existing database structure. In this case we see that the team has chosen to create a column in their spreadsheet called `Site.name`, it is linked to the Neotoma table/column `ndb.sites.sitename`. We state that it does not have to meet a fixed vocabulary, the value repeats within the column, it is expected to be a `string` (as opposed to an `integer` or `numeric` value) and the order of the values does not matter.

A complete list of Neotoma tables and columns is included in [`tablecolumns.csv`](tablecolumns.csv), and additional support for table concepts and content can be found either in the [Neotoma Paleoecology Database Manual](https://open.neotomadb.org/manual) or in the [online database schema](https://open.neotomadb.org/dbschema).

Using the YAML template we can create complex relationships between existing data models for particular sets of records coming from individual researcher labs or data consortiums and the Neotoma database.

On completion of the YAML file, each column of the CSV will have an entry that fully describes the content of the data within that column. At that point we can validate the CSV files intended for upload.

## Validation

We execute the validation process by running:

```bash
> python3 template_validate.py FILEFOLDER
```

This will then search the folder provided in `FILEFOLDER` for csv files and parse them for validity.

The set of tests for validity depends on the data content within the YAML file, but must at least include:

* Site Validation
* Collection Unit Validation
* Analysis Unit Validation
* Dataset Validation
* Dataset PI Validation
* Sample Validation
* Data Validation

Templates with more elements will be tested depending on the data content provided.

Each file will recieve a `log` file associated with it that contains a report of potential issues.

The log files begin with an md5 hash of the csv template file. This appears as a string of numbers and letters that record a point in time of the file. The hash is used to identify whether or not files have changed since validation.

The validation step identifies each element of the template being validated, provides a visual reference as to whether or not the element passes validation (**✔**, **?** or **✗**) and provides guidance as to whether changes need to be made.

## Upload

The upload process is initiated using the command:

```bash
> python3 template_upload.py
```

The upload process will return the distince siteids, and related data identifiers for the uploads.
